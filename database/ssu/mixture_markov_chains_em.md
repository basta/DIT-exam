---
id: ssu_batch4_003
course: Statistical Machine Learning
tags: [markov-chain, mixture-model, em-algorithm, discrete-states]
difficulty: 5
type: derivation
status: to_learn
---

# Question
(6p) Consider the following mixture model for sequences $s = (s_1, \dots, s_n)$ of discrete states $s_i \in K$
$$ p(s) = \pi p_1(s) + (1-\pi) p_2(s), $$
where $p_{1/2}$ are homogeneous Markov models
$$ p_{1/2}(s) = p_{1/2}(s_1) \prod_{i=2}^n p_{1/2}(s_i \mid s_{i-1}). $$
Neither the mixture coefficient $\pi$ nor the parameters of the two Markov models are known. You are given an i.i.d. training set $\mathcal{T}^m = \{s^j \in K^n \mid j = 1, \dots, m\}$ of sequences. Explain how to learn all parameters of the mixture by an EM algorithm.
Recall that the parameters of a homogeneous Markov model are the probabilities of the first state $p(s_1 = k)$ and the matrix of position independent transition probabilities $p(s_i = k \mid s_{i-1} = k')$.

---
# Solution
We have parameters:
- $\pi \in [0, 1]$ (mixture weight)
- $\theta_1 = \{p_1(k), A_1(k' \to k)\}$ (Initial and transition probs for Model 1)
- $\theta_2 = \{p_2(k), A_2(k' \to k)\}$ (Initial and transition probs for Model 2)

**E-step:**
Compute the responsibilities $\gamma_j$: probability that sequence $s^j$ was generated by Model 1, given current parameters.
$$ \gamma_j^{(t)} = p(z_j=1 \mid s^j; \theta^{(t)}) = \frac{\pi^{(t)} p_1(s^j; \theta_1^{(t)})}{\pi^{(t)} p_1(s^j; \theta_1^{(t)}) + (1-\pi^{(t)}) p_2(s^j; \theta_2^{(t)})} $$

**M-step:**
Update parameters by maximizing the expected log-likelihood.
$$ Q = \sum_{j=1}^m [\gamma_j^{(t)} \log(\pi p_1(s^j)) + (1-\gamma_j^{(t)}) \log((1-\pi) p_2(s^j))] $$

1.  **Update $\pi$:**
    $$ \pi^{(t+1)} = \frac{1}{m} \sum_{j=1}^m \gamma_j^{(t)} $$

2.  **Update Model 1 Parameters ($\theta_1$):**
    Maximize $\sum_{j=1}^m \gamma_j^{(t)} \log p_1(s^j)$.
    $$ \log p_1(s^j) = \log p_1(s_1^j) + \sum_{i=2}^n \log p_1(s_i^j \mid s_{i-1}^j) $$
    
    -   **Initial probabilities:** $p_1(k) = P(s_1 = k)$
        $$ p_1^{(t+1)}(k) = \frac{\sum_{j=1}^m \gamma_j^{(t)} \cdot \mathbb{I}[s_1^j = k]}{\sum_{j=1}^m \gamma_j^{(t)}} $$
    
    -   **Transition probabilities:** $A_1(u, v) = P(s_i = v \mid s_{i-1} = u)$
        We count transitions $u \to v$ in all sequences, weighted by $\gamma_j^{(t)}$.
        $$ A_1^{(t+1)}(u, v) = \frac{\sum_{j=1}^m \gamma_j^{(t)} \sum_{i=2}^n \mathbb{I}[s_{i-1}^j=u, s_i^j=v]}{\sum_{j=1}^m \gamma_j^{(t)} \sum_{i=2}^n \mathbb{I}[s_{i-1}^j=u]} $$

3.  **Update Model 2 Parameters ($\theta_2$):**
    Analogous to Model 1, using weights $(1-\gamma_j^{(t)})$.
    $$ p_2^{(t+1)}(k) = \frac{\sum_{j=1}^m (1-\gamma_j^{(t)}) \cdot \mathbb{I}[s_1^j = k]}{\sum_{j=1}^m (1-\gamma_j^{(t)})} $$
    $$ A_2^{(t+1)}(u, v) = \frac{\sum_{j=1}^m (1-\gamma_j^{(t)}) \sum_{i=2}^n \mathbb{I}[s_{i-1}^j=u, s_i^j=v]}{\sum_{j=1}^m (1-\gamma_j^{(t)}) \sum_{i=2}^n \mathbb{I}[s_{i-1}^j=u]} $$

## Related Concepts
- [[em-algorithm]]
- [[mixture-model]]
- [[markov-chain]]
- [[maximum-likelihood]]
