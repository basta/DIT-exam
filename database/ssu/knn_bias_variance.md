---
id: ssu_batch6_006
course: Statistical Machine Learning
tags: [bias-variance, k-nearest-neighbors, regression, error-decomposition]
difficulty: 5
type: derivation
status: to_learn
---

# Question
(4p) Consider a regression problem with multiple training datasets $\mathcal{T}^m = \{(x_i, y_i) \mid i = 1, \dots, m\}$ of size $m$ generated by using
$$ y = f(x) + \epsilon, \quad (1) $$
where $\epsilon$ is noise with $\mathbb{E}(\epsilon) = 0$ and $\text{Var}(\epsilon) = \sigma^2$. Derive the bias-variance decomposition for the 1-nearest-neighbor regression. The response of the 1-NN regressor is defined as:
$$ h_m(x) = y_{n(x)} = f(x_{n(x)}) + \epsilon, $$
where $n(x)$ gives the index of the nearest neighbor of $x$ in $\mathcal{T}^m$. For simplicity assume that all $x_i$ are the same for all training datasets $\mathcal{T}^m$ in consideration, hence, the randomness arises from the noise $\epsilon$, only.
Give the squared bias:
$$ \mathbb{E}_x \left[ (g_m(x) - f(x))^2 \right] = \mathbb{E}_x \left[ (\mathbb{E}_{\mathcal{T}^m}(h_m(x)) - f(x))^2 \right] $$
and variance:
$$ \text{Var}_{x, \mathcal{T}^m}(h_m(x)). $$

---
# Solution
We analyze the estimator at a fixed query point $x$.
The nearest neighbor $x_{n(x)}$ is fixed because the inputs $x_i$ are fixed across datasets (only $y$ varies due to noise). So $n(x)$ is deterministic.
Let $x_{nn} = x_{n(x)}$.
The estimator is $h_m(x) = y_{nn} = f(x_{nn}) + \epsilon_{nn}$.

## Squared Bias
The expected prediction over datasets (noise realizations):
$$ g_m(x) = \mathbb{E}_{\mathcal{T}^m}[h_m(x)] = \mathbb{E}[f(x_{nn}) + \epsilon_{nn}] = f(x_{nn}) + \mathbb{E}[\epsilon_{nn}] = f(x_{nn}) $$
Bias term (squared):
$$ (\text{Bias})^2 = (g_m(x) - f(x))^2 = (f(x_{nn}) - f(x))^2 $$
This is non-zero unless the function is constant or $x$ is in the training set (infinite density limit).

## Variance
$$ \text{Var}(h_m(x)) = \mathbb{E}[(h_m(x) - g_m(x))^2] $$
$$ = \mathbb{E}[ (f(x_{nn}) + \epsilon_{nn} - f(x_{nn}))^2 ] $$
$$ = \mathbb{E}[ \epsilon_{nn}^2 ] = \sigma^2 $$
So the variance of 1-NN is $\sigma^2$ (it doesn't decrease with $m$ because we average over 1 neighbor).

## Summary
-   **Bias$^2$:** $(f(x_{nn}) - f(x))^2$
-   **Variance:** $\sigma^2$

## Related Concepts
- [[bias-variance]]
- [[nearest-neighbor]]
- [[regression]]
